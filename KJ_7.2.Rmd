---
title: KJ 7.2
author: Heather Geiger
output:
 html_document:
  smart: false
---

# Libraries

Load libraries.

```{r,message=FALSE,warning=FALSE}
library(mlbench)
library(caret)
library(mda)
library(ggplot2)
library(tidyr)
library(dplyr)
```

# Set up question data.

Set seed.

```{r}
set.seed(200)
```

Set up training data.

```{r}
trainingData <- mlbench.friedman1(200, sd = 1)
```

Format training data.

```{r}
trainingData$x <- data.frame(trainingData$x)
```

Plot training data predictors vs. outcome.

```{r}
featurePlot(trainingData$x,trainingData$y)
```

Set up and format test data.

```{r}
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

# Instructions

Tune several models on these data.

Example of a model given was using the caret package train function with a KNN model, pre-process training data with centering and scaling, tuneLength = 10 (for KNN, this would mean trying 10 different values for k).

Then, use the models based on the training data to predict on test data. 

Which models appear to give the best performance? Does MARS select the informative predictors (those named X1â€“X5)?

# Answer

## Neural networks

Let's try the neural networks method with model averaging.

```{r avg-nnet,echo=TRUE,eval=FALSE}
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                .size = c(1:10),
                .bag = FALSE)

averaging_nnet_model <- train(trainingData$x,trainingData$y,
                            method="avNNet",
                            tuneGrid = nnetGrid,
                            trControl = trainControl(method = "cv", number = 10),
                            preProc = c("center", "scale"),
                            linout = TRUE,
                            trace=FALSE,
                            maxit = 500)
```

```{r,echo=FALSE,eval=FALSE}
save(averaging_nnet_model,
    file="averaging_nnet_model.Rdata")
```

```{r,echo=FALSE,eval=TRUE}
load("averaging_nnet_model.Rdata")
```

Look at error and R-squared of this model applied to test data.

```{r}
predictions <- predict(averaging_nnet_model,newdata = testData$x)

postResample(pred = predictions, obs = testData$y)
```

## Multivariate Adaptive Regression Splines (MARS)

Was going to use bagEarth from the caret package, but can't get dependency earth to install.

So holding off on this section for now.

```{r,echo=TRUE,eval=FALSE}
MARS_model <- bagEarth(trainingData$x,trainingData$y)
```

Print model details.

```{r,echo=TRUE,eval=FALSE}
MARS_model

summary(MARS_model)
```

Look at error and R-squared of this model applied to test data.

```{r,echo=TRUE,eval=FALSE}
predictions <- predict(MARS_model,newdata = testData$x)

postResample(pred = predictions, obs = testData$y)
```

##  Support Vector Machine (SVM)

Let's use method="svmRadial" within the train function.

```{r,message=FALSE,warning=FALSE}
SVM_model <- train(trainingData$x,trainingData$y,
                method="svmRadial",
                trControl = trainControl(method = "cv", number = 10),
                preProc = c("center", "scale"),
                tuneLength = 14)
```

```{r,echo=FALSE,eval=TRUE}
save(SVM_model,
    file="SVM_model.Rdata")
```

```{r,echo=FALSE,eval=FALSE}
load("SVM_model.Rdata")
```

Look at error and R-squared of this model applied to test data.

```{r}
predictions <- predict(SVM_model,newdata = testData$x)

postResample(pred = predictions, obs = testData$y)
```

Ooh, that's not great at all. 

Try linear kernel instead.

```{r,message=FALSE,warning=FALSE}
SVM_model2 <- train(trainingData$x,trainingData$y,
    method="svmLinear",
    trControl = trainControl(method = "cv", number = 10),
    preProc = c("center", "scale"),
    tuneLength = 14)
```

```{r,echo=FALSE,eval=TRUE}
save(SVM_model2,file="SVM_linear.Rdata")
```

```{r,echo=FALSE,eval=FALSE}
load("SVM_linear.Rdata")
```

```{r}
predictions <- predict(SVM_model2,newdata = testData$x)

postResample(pred = predictions, obs = testData$y)
```

This is still not quite as good as the neural net, but it is a lot better than the radial basis function kernel.

Next, I was going to try the polynomial kernel, but runtime was impractically long. So let's move on.

# K-Nearest Neighbors (KNN)

Let's run KNN just as it was run in the question example.

```{r}
knnModel <- train(x = trainingData$x,
                y = trainingData$y,
                method = "knn",
                preProc = c("center", "scale"),
                tuneLength = 10)
```

```{r}
predictions <- predict(knnModel,newdata = testData$x)

postResample(pred = predictions, obs = testData$y)
```

# Comparing models

Based on the error terms (RMSE/MAE) and the R-squared value, looks like the neural network model with model averaging had the best performance by far, assuming MARS doesn't end up being even better.

#  Does MARS select the informative predictors?

Holding off on this section until deal with installation issue.

